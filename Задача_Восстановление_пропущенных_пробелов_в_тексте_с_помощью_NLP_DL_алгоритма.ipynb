{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "file_path = 'corpus.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "def train_model(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    unigram_counts = defaultdict(int)\n",
        "    bigram_counts = defaultdict(int)\n",
        "\n",
        "    total_words = len(words)\n",
        "\n",
        "    # Подсчет униграмм\n",
        "    for word in words:\n",
        "        unigram_counts[word] += 1\n",
        "\n",
        "    # Подсчет биграмм\n",
        "    if total_words > 1:\n",
        "        for i in range(total_words - 1):\n",
        "            bigram_counts[(words[i], words[i+1])] += 1\n",
        "\n",
        "    # Подсчет частоты символов для обработки неизвестных слов\n",
        "    char_unigram_counts = defaultdict(int)\n",
        "    for char in text:\n",
        "        if 'а' <= char.lower() <= 'я':\n",
        "            char_unigram_counts[char.lower()] += 1\n",
        "\n",
        "    total_chars = sum(char_unigram_counts.values())\n",
        "\n",
        "    # Вычисление вероятностей (сглаживание Лапласа)\n",
        "    vocab_size = len(unigram_counts)\n",
        "    unigram_probs = {word: (count + 1) / (total_words + vocab_size)\n",
        "                     for word, count in unigram_counts.items()}\n",
        "\n",
        "    bigram_probs = {\n",
        "        (w1, w2): (count + 1) / (unigram_counts[w1] + vocab_size)\n",
        "        for (w1, w2), count in bigram_counts.items()\n",
        "    }\n",
        "\n",
        "    char_unigram_probs = {\n",
        "        char: (count + 1) / (total_chars + len(char_unigram_counts))\n",
        "        for char, count in char_unigram_counts.items()\n",
        "    }\n",
        "\n",
        "    return unigram_probs, bigram_probs, char_unigram_probs, set(unigram_counts.keys())\n",
        "\n",
        "UNIGRAM_PROBS, BIGRAM_PROBS, CHAR_UNIGRAM_PROBS, VOCABULARY = train_model(text)\n",
        "\n",
        "def get_word_prob(word, prev_word, unigram_probs, bigram_probs, char_probs):\n",
        "    prob = 1e-10\n",
        "\n",
        "    if prev_word is not None and (prev_word, word) in bigram_probs:\n",
        "        prob = bigram_probs[(prev_word, word)]\n",
        "    elif word in unigram_probs:\n",
        "        prob = unigram_probs[word]\n",
        "    else:\n",
        "        for char in word:\n",
        "            prob *= char_probs.get(char, 1e-10)\n",
        "\n",
        "    return math.log(prob)\n",
        "\n",
        "def reconstruct_text(text_without_spaces, unigram_probs, bigram_probs, char_probs, vocabulary):\n",
        "    \"\"\"\n",
        "    Основной алгоритм восстановления текста.\n",
        "    Использует динамическое программирование для поиска наиболее вероятного разбиения.\n",
        "    \"\"\"\n",
        "    n = len(text_without_spaces)\n",
        "    dp = [(-math.inf, -1, None)] * (n + 1)\n",
        "    dp[0] = (0, -1, None)\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(i):\n",
        "            word = text_without_spaces[j:i]\n",
        "\n",
        "            prev_word = dp[j][2]\n",
        "\n",
        "            log_prob = get_word_prob(word, prev_word, unigram_probs, bigram_probs, char_probs)\n",
        "\n",
        "            if dp[j][0] != -math.inf:\n",
        "                current_log_prob = dp[j][0] + log_prob\n",
        "                if current_log_prob > dp[i][0]:\n",
        "                    dp[i] = (current_log_prob, j, word)\n",
        "\n",
        "    if dp[n][0] == -math.inf:\n",
        "        return \"Не удалось восстановить текст.\"\n",
        "\n",
        "    result_words = []\n",
        "    current_index = n\n",
        "    while current_index > 0:\n",
        "        log_prob, prev_index, word = dp[current_index]\n",
        "        result_words.append(word)\n",
        "        current_index = prev_index\n",
        "\n",
        "    return ' '.join(reversed(result_words))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    texts_to_process = [\"приветкакдела\"]\n",
        "\n",
        "\n",
        "    for text in texts_to_process:\n",
        "        result = reconstruct_text(text, UNIGRAM_PROBS, BIGRAM_PROBS, CHAR_UNIGRAM_PROBS, VOCABULARY)\n",
        "        print(f\"Входной текст: '{text}'\")\n",
        "        print(f\"Восстановленный текст: '{result}'\")"
      ],
      "metadata": {
        "id": "fEDFll4cc7Nu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}